---
title: "A Benchmark Analysis of Online Footwear Companies Experiences"
author: "Dan Freeman"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

### Data Source

- Using Google's BigQuery interface at https://console.cloud.google.com/bigquery - query the crux data set
- Dataset: `chrome-ux-report`


There are a number of tables within the `chrome-ux-report`, many have a histogram type layout and are very data intensive.  

Given that, data quotas were met, therefore the scope of this report was adjusted to what could be queried within the free tier.

Looking at the `materialized` table and by `country_summary`, there was enough useful information to make a reasonably helpful report in R

### Steps

- get data from BigQuery, repeat sql states, as shown above, for each month (note data is aggregated by month in BigQuery)
- export each query result to a .csv file
- in **R Studio**: create datasets and show a series of  graphs to demonstrate how different companies rank against each other. 
- goal: show how the user experiences differ from site to site.

### Cavets

- This data is from Jan, Feb, March of 2004
- Data is origin based, meaning it is an aggregate of all pages and subpages throughout the entire domain


```{r setup env, echo=FALSE, message=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com"))
install.packages("ggplot2", quiet = TRUE)  # For plotting
install.packages("dplyr", quiet = TRUE)    # For data manipulation
install.packages("lubridate", quiet = TRUE)
install.packages("tinytex", quiet = TRUE)
library(ggplot2)
library(dplyr)
library(lubridate)
library(tidyr)
library(tinytex)
```

```{r setup data, echo=FALSE}
january_data <- read.csv("data/all-202401.csv")
february_data <- read.csv("data/all-202402.csv")
march_data <- read.csv("data/all-202403.csv")


all_data <- rbind(january_data, february_data, march_data)
```

```{r clean data, echo=FALSE}
# clean data, we only want to deal with desktop and companies that have 3 months of data
all_data <- all_data %>% filter(device != "phone") 
all_data <- all_data %>% filter(device != "tablet") 
all_data <- all_data %>% filter(origin != "https://www.on.com") 
all_data <- all_data %>% filter(origin != "https://www.hoka.com") 


# add an actual date column to use as an x-axis
all_data$date_type <- ym(all_data$yyyymm)
```


## Starting with the First Contentful Paint, we get our first glance on how these companies compare

```{r show fcp, echo=FALSE, warning=FALSE}
ggplot(all_data, aes(x=date_type, y=p75_fcp, group=origin, color=origin)) +
  geom_line() +
  theme(
    legend.position = "bottom",
    plot.background = element_rect(fill = "white", color = NA), 
    panel.background = element_rect(fill = "white", color = NA),
    legend.text = element_text(size = 11, family = "Times", face = "bold")) +
  guides(color = guide_legend(ncol = 2)) +
  geom_line(size = 1.5) + # Thicker lines  
  scale_x_date(date_labels = "%B %Y", date_breaks = "1 month") +
  labs(
    x = "Month",
    y = "FCP",
    title = "First Contentful Paint - p75th"
  ) 
```

## Next we look at Largest Contentful Paint, we can see the rankings remain fairly similar

```{r show lcp, echo=FALSE, warning=FALSE}
ggplot(all_data, aes(x=date_type, y=p75_lcp, group=origin, color=origin)) +
  geom_line() +
  theme(
    legend.position = "bottom",
    plot.background = element_rect(fill = "white", color = NA), 
    panel.background = element_rect(fill = "white", color = NA),
    legend.text = element_text(size = 11, family = "Times", face = "bold")) +
  guides(color = guide_legend(ncol = 2)) +
  geom_line(size = 1.5) + # Thicker lines
  scale_x_date(date_labels = "%B %Y", date_breaks = "1 month") +
  labs(
    x = "Month",
    y = "LCP",
    title = "Largest Contentful Paint - p75th"
  ) 
```

## Moving to Time to First Byte, we see a few deviations from the previous graphs, however it's making us want to dig deeper to explain the why LCP scores differ

```{r show ttfb, echo=FALSE, warning=FALSE}  
ggplot(all_data, aes(x=date_type, y=p75_ttfb, group=origin, color=origin)) +
  geom_line() +
  theme(
    legend.position = "bottom",
    plot.background = element_rect(fill = "white", color = NA), 
    panel.background = element_rect(fill = "white", color = NA),
    legend.text = element_text(size = 11, family = "Times", face = "bold")) +
  guides(color = guide_legend(ncol = 2)) +
  geom_line(size = 1.5) + # Thicker lines
  scale_x_date(date_labels = "%B %Y", date_breaks = "1 month") +
  labs(
    x = "Month",
    y = "TTFB",
    title = "Time to First Byte - p75th"
  ) 
```

```{r setup average data, echo=FALSE}
all_average_data <- all_data %>%
  group_by(origin) %>%
  summarise(avg_value = mean(desktopDensity))

# Lookup function to rename groups
lookup <- function(x) {
  lookup_table <- c("https://www.adidas.com" = "Adidas", "https://www.newbalance.com" = "New Balance", "https://www.nike.com" = "Nike", "https://www.underarmour.com" = "Under Armour")
  return(lookup_table[x])
}
```

## For the fun of it, let's look at the percentage of users that were on desktop by company, we can assume the rest of the users were on mobile.
## We can see that Under Armour's users definitely prefer mobile web.

```{r bar plot, echo=FALSE, warning=FALSE}
# Create the bar plot
ggplot(all_average_data, aes(x = origin, y = avg_value, fill = origin)) +
  geom_bar(stat = "identity") +
  scale_x_discrete(labels = function(x) lookup(x)) + 
  labs(title = "Percent of Desktop Users", x = "Origin", y = "Percentage") +
  theme_minimal()
```

## Digging further into the LCP metric, this shows the precentage of users that experience a 'Fast LCP', meaning the LCP score was under 2.5 seconds.
## The faster the p75 LCP, we would assume that we'd have more users in the 'Fast LCP' bucket
## This holds true for New Balance, it's fast and they have a big Fast LCP bucket
## Conversely for Nike, it's rather slow and thusly a small LCP bucket
## Adidas in the the middle for both p75 LCP and, as predicted, the Fast LCP bucket
## But what is going on with Under Armour, it's LCP is good but it has the *smallest* Fast LCP bucket

```{r grouped data, echo=FALSE}
all_grouped_average_data <- all_data %>%
  group_by(origin) %>%
  summarise(
    avg_fast_lcp = mean(fast_lcp)*10000,
    avg_p75_lcp = mean(p75_lcp)
  )

all_grouped_average_data_long <- all_grouped_average_data %>%
  gather(key = "variable", value = "mean_value", avg_fast_lcp, avg_p75_lcp)


min_value2 <- min(all_grouped_average_data$avg_fast_lcp)
max_value2 <- max(all_grouped_average_data$avg_fast_lcp)

```




```{r bar grouped data, echo=FALSE, warning=FALSE}
# Create the bar plot
ggplot(all_grouped_average_data_long, aes(x = origin, y = mean_value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +  # Position bars side by side
  scale_x_discrete(labels = function(x) lookup(x)) +  # Rename x-axis labels using the lookup function
  labs(title = "LCP vs Percentage of Fast LCP", x = "Group", y = "Average Value") +
  scale_y_continuous(
    name = "Average LCP", 
    sec.axis = sec_axis(
      trans = ~ . / 10000, 
      name = "Average Percentage of Fast LCP"
    )  # Secondary y-axis for 'avg_value2'
  ) +
  scale_fill_manual(
    values = c("avg_fast_lcp" = "royalblue4", "avg_p75_lcp" = "salmon2"),
    labels = c("Fast LCP Percent", "LCP")
  ) +   
  theme_minimal()

```

```{r setup 4g, echo=FALSE}
all_average_4g_data <- all_data %>%
  group_by(origin) %>%
  summarise(avg_value = mean(X_4GDensity))
```

## Exploring more RUM metrics, we can inspect the percetage of users that are on 4G networks or better
## Under Armour has the smallest amount of 4G users.  This would explain the smaller Fast LCP bucket shown in the previous bar graph.
## It would also infer that Under Armour would have an even better overall LCP score if they had a higher percentage of users utilizing fast networks.

```{r final 4g, echo=FALSE, warning=FALSE}
# Create the bar plot
ggplot(all_average_4g_data, aes(x = origin, y = avg_value, fill = origin)) +
  geom_bar(stat = "identity") +
  scale_x_discrete(labels = function(x) lookup(x)) + 
  labs(title = "Percentage of 4g Users", x = "Origin", y = "Percentage") +
  theme_minimal()

```